{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('base': conda)",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fdce1bee3a4df8d1a1fa4d38b29664ec97e28d1bea539720d01ec921b2621e44"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize, RegexpTokenizer, regexp_tokenize, WhitespaceTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.book import *\n",
    "import spacy\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from nlp_cleaning import *\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"attraction_point_reviews.csv\")\n",
    "# df.tail()"
   ]
  },
  {
   "source": [
    "# Cleaning the Text \n",
    "\n",
    "As part of cleaning the text before tokenizing it, various steps will be undertaken. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### But first, to get an initial feel for the data, the following steps are being undertaken:\n",
    "1. All reviews are combined into a giant single string corpus.\n",
    "1. All words are changed to lower letter case\n",
    "1. website links & email ids are dropped\n",
    "1. A lot of words are wrongly connected with punctuations. Simply dropping the punctuations will connect these words. These will be substituted with whitespace for the following punctuations: <.*?>;-!()/,:&—\\ \n",
    "1. Everything except for letters & whitespace is dropped with no substitutions in between.\n",
    "1. The string is tokenized on whitespace.\n",
    "1. It is then converted into a word counter using FreqDist to explore frequencies and look at the various words used."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_reviews = ' '.join(df.review_text.tolist())\n",
    "all_reviews = all_reviews.lower()\n",
    "all_reviews = re.sub('http\\S+', '' , all_reviews)\n",
    "all_reviews = re.sub('\\S*@\\S+', '', all_reviews)\n",
    "\n",
    "all_reviews = re.sub(r'[<.*?>;\\-!()/,:&—\\\\]+', ' ', all_reviews)\n",
    "all_reviews = re.sub(r'[^A-Za-z\\s]', '', all_reviews)\n",
    "words = WhitespaceTokenizer().tokenize(all_reviews)\n",
    "words_count = FreqDist(words)\n",
    "\n",
    "long_words = [w for w in words_count if len(w) > 15]\n",
    "small_words = [w for w in words_count if len(w) < 4]"
   ]
  },
  {
   "source": [
    "**The above process was carried out iteratively to ensure as many long words as possible can be captured properly and not words arbitrarily combined together using punctuations**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## COME BACK TO TRY MORE CLEANING FOR:\n",
    "1. letter repeats\n",
    "2. Spell Check\n",
    "1. Different languages\n",
    "1. Named Entity Extraction\n",
    "3. Identifying actual hypentated words instead of separating them \n",
    "1. Removing words that are less than 4 letters perhaps - but selectively"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The cleaning strategy was applied to the "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6958    teens left mom and dad in the dust    we took frequent stops due to the steep paved walkway   take plenty of water     best to visit early in the morning to avoid the crowds   be aware not to venture past the railings  warning signs  etc    this area has had four deaths this summer   stay out of the water period   stay off the rocks period   one  yr old teen died on aug   due to a fall on the rocks take no strollers or wheelchairs    not a venture for those in a wheelchair   travel light and wear proper walking shoes   do not wear your flip flops or sandals \nName: review_clean, dtype: object"
     },
     "metadata": {},
     "execution_count": 348
    }
   ],
   "source": [
    "df['review_clean'] = df.review_text.map(cleaning)\n",
    "df.review_clean.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en', disable=[ 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "yes   have be here many time    once propose to -PRON- now wife at sunrise    be one of the most amazing view in the world   up there with the grand canyon    s not to like note    in either   or   the park service will close glacier point road   be postpone from    the only vehicle access   for a year or more in order to renovate    so go now\nyes  weve been here many times   once i proposed to my now wife at sunrise   it is one of the most amazing views in the world  up there with the grand canyon   whats not to like note   in either  or  the park service will close glacier point road  it was postponed from   the only vehicle access  for a year or more in order to renovate it   so go now \n[&#39;INTJ&#39;, &#39;SPACE&#39;, &#39;PRON&#39;, &#39;VERB&#39;, &#39;AUX&#39;, &#39;ADV&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;SPACE&#39;, &#39;SCONJ&#39;, &#39;PRON&#39;, &#39;VERB&#39;, &#39;ADP&#39;, &#39;DET&#39;, &#39;ADV&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;NOUN&#39;, &#39;SPACE&#39;, &#39;PRON&#39;, &#39;AUX&#39;, &#39;NUM&#39;, &#39;ADP&#39;, &#39;DET&#39;, &#39;ADV&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;ADP&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;SPACE&#39;, &#39;ADV&#39;, &#39;ADV&#39;, &#39;ADP&#39;, &#39;DET&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;SPACE&#39;, &#39;PRON&#39;, &#39;VERB&#39;, &#39;PART&#39;, &#39;PART&#39;, &#39;INTJ&#39;, &#39;NOUN&#39;, &#39;SPACE&#39;, &#39;ADP&#39;, &#39;DET&#39;, &#39;SPACE&#39;, &#39;CCONJ&#39;, &#39;SPACE&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;VERB&#39;, &#39;VERB&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;SPACE&#39;, &#39;PRON&#39;, &#39;AUX&#39;, &#39;VERB&#39;, &#39;ADP&#39;, &#39;SPACE&#39;, &#39;DET&#39;, &#39;ADJ&#39;, &#39;NOUN&#39;, &#39;NOUN&#39;, &#39;SPACE&#39;, &#39;ADP&#39;, &#39;DET&#39;, &#39;NOUN&#39;, &#39;CCONJ&#39;, &#39;ADJ&#39;, &#39;ADP&#39;, &#39;NOUN&#39;, &#39;PART&#39;, &#39;VERB&#39;, &#39;PRON&#39;, &#39;SPACE&#39;, &#39;ADV&#39;, &#39;VERB&#39;, &#39;ADV&#39;]\n[autoreload of nlp_cleaning failed: Traceback (most recent call last):\n  File &quot;C:\\Users\\Navish\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py&quot;, line 245, in check\n    superreload(m, reload, self.old_objects)\n  File &quot;C:\\Users\\Navish\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py&quot;, line 394, in superreload\n    module = reload(module)\n  File &quot;C:\\Users\\Navish\\anaconda3\\lib\\imp.py&quot;, line 314, in reload\n    return importlib.reload(module)\n  File &quot;C:\\Users\\Navish\\anaconda3\\lib\\importlib\\__init__.py&quot;, line 169, in reload\n    _bootstrap._exec(spec, module)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 604, in _exec\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 783, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;c:\\Users\\Navish\\Dropbox\\Metis\\Projects\\Project_4\\Trip_Advisor_National_Parks\\nlp_cleaning.py&quot;, line 23, in &lt;module&gt;\n    nlp = spacy.load(&#39;en_core_web_sm&#39;, disable=[ &#39;parser&#39;, &#39;ner&#39;])\n  File &quot;C:\\Users\\Navish\\anaconda3\\lib\\site-packages\\spacy\\__init__.py&quot;, line 30, in load\n    return util.load_model(name, **overrides)\n  File &quot;C:\\Users\\Navish\\anaconda3\\lib\\site-packages\\spacy\\util.py&quot;, line 175, in load_model\n    raise IOError(Errors.E050.format(name=name))\nOSError: [E050] Can&#39;t find model &#39;en_core_web_sm&#39;. It doesn&#39;t seem to be a shortcut link, a Python package or a valid path to a data directory.\n]\n"
    }
   ],
   "source": [
    "n = 7\n",
    "spacy_text = sp_nlp(df.review_clean[n])\n",
    "text_lemma = [word.lemma_ for word in spacy_text \\\n",
    "                    if word.pos_ != 'PRON']\n",
    "print(' '.join(text_lemma))\n",
    "print(df.review_clean[n])\n",
    "\n",
    "print([word.pos_ for word in spacy_text])\n",
    "# ' '.join(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "source": [
    "### Tokenization "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[&#39;Hi&#39;, &#39;Mr.&#39;, &#39;Smith&#39;, &#39;!&#39;, &#39;I&#39;, &#39;’&#39;, &#39;m&#39;, &#39;going&#39;, &#39;to&#39;, &#39;buy&#39;, &#39;some&#39;, &#39;vegetables&#39;, &#39;(&#39;, &#39;tomatoes&#39;, &#39;and&#39;, &#39;cucumbers&#39;, &#39;)&#39;, &#39;from&#39;, &#39;the&#39;, &#39;store&#39;, &#39;.&#39;, &#39;Should&#39;, &#39;I&#39;, &#39;pick&#39;, &#39;up&#39;, &#39;some&#39;, &#39;black-eyed&#39;, &#39;peas&#39;, &#39;as&#39;, &#39;well&#39;, &#39;?&#39;]\n[(&#39;Hi&#39;, &#39;Mr.&#39;), (&#39;Mr.&#39;, &#39;Smith&#39;), (&#39;Smith&#39;, &#39;!&#39;), (&#39;!&#39;, &#39;I&#39;), (&#39;I&#39;, &#39;’&#39;), (&#39;’&#39;, &#39;m&#39;), (&#39;m&#39;, &#39;going&#39;), (&#39;going&#39;, &#39;to&#39;), (&#39;to&#39;, &#39;buy&#39;), (&#39;buy&#39;, &#39;some&#39;), (&#39;some&#39;, &#39;vegetables&#39;), (&#39;vegetables&#39;, &#39;(&#39;), (&#39;(&#39;, &#39;tomatoes&#39;), (&#39;tomatoes&#39;, &#39;and&#39;), (&#39;and&#39;, &#39;cucumbers&#39;), (&#39;cucumbers&#39;, &#39;)&#39;), (&#39;)&#39;, &#39;from&#39;), (&#39;from&#39;, &#39;the&#39;), (&#39;the&#39;, &#39;store&#39;), (&#39;store&#39;, &#39;.&#39;), (&#39;.&#39;, &#39;Should&#39;), (&#39;Should&#39;, &#39;I&#39;), (&#39;I&#39;, &#39;pick&#39;), (&#39;pick&#39;, &#39;up&#39;), (&#39;up&#39;, &#39;some&#39;), (&#39;some&#39;, &#39;black-eyed&#39;), (&#39;black-eyed&#39;, &#39;peas&#39;), (&#39;peas&#39;, &#39;as&#39;), (&#39;as&#39;, &#39;well&#39;), (&#39;well&#39;, &#39;?&#39;)]\n[&#39;Hi&#39;, &#39;Mr.&#39;, &#39;Smith!&#39;, &#39;I’m&#39;, &#39;going&#39;, &#39;to&#39;, &#39;buy&#39;, &#39;some&#39;, &#39;vegetables&#39;, &#39;(tomatoes&#39;, &#39;and&#39;, &#39;cucumbers)&#39;, &#39;from&#39;, &#39;the&#39;, &#39;store.&#39;, &#39;Should&#39;, &#39;I&#39;, &#39;pick&#39;, &#39;up&#39;, &#39;some&#39;, &#39;black-eyed&#39;, &#39;peas&#39;, &#39;as&#39;, &#39;well?&#39;]\n[&#39;Hi&#39;, &#39;Mr&#39;, &#39;Smith&#39;, &#39;Should&#39;]\n"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from \\\n",
    "the store. Should I pick up some black-eyed peas as well?\"\n",
    "\n",
    "print(word_tokenize(my_text))\n",
    "\n",
    "# (N-Grams)\n",
    "\n",
    "from nltk.util import ngrams\n",
    "my_words = word_tokenize(my_text) # This is the list of all words\n",
    "twograms = list(ngrams(my_words,2)) # This is for two-word combos, but can pick any n\n",
    "print(twograms)\n",
    "\n",
    "# Regular Expressions\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# RegexpTokenizer with whitespace delimiter\n",
    "whitespace_tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "print(whitespace_tokenizer.tokenize(my_text))\n",
    "\n",
    "# RegexpTokenizer to match only capitalized words\n",
    "cap_tokenizer = RegexpTokenizer(\"[A-Z]['\\w]+\")\n",
    "print(cap_tokenizer.tokenize(my_text))\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "regexp_tokenize(s, pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "wordpunct_tokenize(s)\n",
    "\n",
    "blankline_tokenize(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Preprocessing: Stop Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "#Example impact with code\n",
    "\n",
    "my_text = [\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from \\\n",
    "the store. Should I pick up some black-eyed peas as well?\"]\n",
    "\n",
    "# Incorporate stop words when creating the count vectorizer\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(my_text)\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "source": [
    "### POS Tagging With NLTK"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(&#39;James&#39;, &#39;NNP&#39;), (&#39;Smith&#39;, &#39;NNP&#39;), (&#39;lives&#39;, &#39;VBZ&#39;), (&#39;in&#39;, &#39;IN&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;United&#39;, &#39;NNP&#39;), (&#39;States&#39;, &#39;NNPS&#39;), (&#39;.&#39;, &#39;.&#39;)]\n"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text))\n",
    "print(tokens)\n",
    "\n",
    "#For help on the codes, use the below\n",
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "source": [
    "### Named Entity Recognition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text)) # this labels each word as a part of speech\n",
    "entities = ne_chunk(tokens) # this extracts entities from the list of words\n",
    "# help(entities)"
   ]
  },
  {
   "source": [
    "### Compound Term Extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer # multi-word expression\n",
    "my_text = \"You all are the greatest students of all time.\"\n",
    "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "mwe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}