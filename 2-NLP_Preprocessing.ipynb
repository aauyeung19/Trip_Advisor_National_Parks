{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('base': conda)",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fdce1bee3a4df8d1a1fa4d38b29664ec97e28d1bea539720d01ec921b2621e44"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize, RegexpTokenizer, regexp_tokenize, WhitespaceTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.book import *\n",
    "import spacy\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from nlp_cleaning import *\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"attraction_point_reviews.csv\")\n",
    "# df.tail()"
   ]
  },
  {
   "source": [
    "# Cleaning the Text \n",
    "\n",
    "As part of cleaning the text before tokenizing it, various steps will be undertaken. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### But first, to get an initial feel for the data, the following steps are being undertaken:\n",
    "1. All reviews are combined into a giant single string corpus.\n",
    "1. All words are changed to lower letter case\n",
    "1. website links & email ids are dropped\n",
    "1. A lot of words are wrongly connected with punctuations. Simply dropping the punctuations will connect these words. These will be substituted with whitespace for the following punctuations: <.*?>;-!()/,:&—\\ \n",
    "1. Everything except for letters & whitespace is dropped with no substitutions in between.\n",
    "1. The string is tokenized on whitespace.\n",
    "1. It is then converted into a word counter using FreqDist to explore frequencies and look at the various words used."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_reviews = ' '.join(df.review_text.tolist())\n",
    "all_reviews = all_reviews.lower()\n",
    "all_reviews = re.sub('http\\S+', '' , all_reviews)\n",
    "all_reviews = re.sub('\\S*@\\S+', '', all_reviews)\n",
    "\n",
    "all_reviews = re.sub(r'[<.*?>;\\-!()/,:&—\\\\]+', ' ', all_reviews)\n",
    "all_reviews = re.sub(r'[^A-Za-z\\s]', '', all_reviews)\n",
    "words = WhitespaceTokenizer().tokenize(all_reviews)\n",
    "words_count = FreqDist(words)\n",
    "\n",
    "long_words = [w for w in words_count if len(w) > 15]\n",
    "small_words = [w for w in words_count if len(w) < 4]"
   ]
  },
  {
   "source": [
    "**The above process was carried out iteratively to ensure as many long words as possible can be captured properly and not words arbitrarily combined together using punctuations**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## COME BACK TO TRY MORE CLEANING FOR:\n",
    "1. letter repeats (tweet tokenizer)\n",
    "1. Spell Check\n",
    "1. Different languages\n",
    "1. Named Entity Extraction\n",
    "3. Identifying actual hypentated words instead of separating them \n",
    "1. Removing words that are less than 4 letters perhaps - but selectively"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**The cleaning strategy was applied to the reviews in the dataframe.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6958    teens left mom and dad in the dust    we took frequent stops due to the steep paved walkway   take plenty of water     best to visit early in the morning to avoid the crowds   be aware not to venture past the railings  warning signs  etc    this area has had four deaths this summer   stay out of the water period   stay off the rocks period   one  yr old teen died on aug   due to a fall on the rocks take no strollers or wheelchairs    not a venture for those in a wheelchair   travel light and wear proper walking shoes   do not wear your flip flops or sandals \nName: review_clean, dtype: object"
     },
     "metadata": {},
     "execution_count": 348
    }
   ],
   "source": [
    "df['review_clean'] = df.review_text.map(cleaning)\n",
    "df.review_clean.sample()"
   ]
  },
  {
   "source": [
    "The words are then lemmatized after cleaning, to reduce the variety of words itself in the corpus. Additionally, all pronouns are dropped."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Wall time: 2min 24s\n"
    }
   ],
   "source": [
    "%%time\n",
    "df['review_lemma'] = df.review_clean.map(spacy_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                                                                                                                                                                                           review_lemma  \\\n2681  the park be huge    realize that  can not be experience in just a couple of day   the pass be magnificent    husband love to do the loop from    to tioga pass and back around to twain harte on  motorcycle   take  about   hour   \n2391  with lake and meadow everywhere this road be the drive in heaven    take  from lee vine to the olmstead point    wish to go further the next time                                                                                   \n\n                                                                                                                                                                                                                                    review_clean  \n2681  the park is huge  you realize that it cant be experience in just a couple of days  the pass is magnificent  my husband loves to do the loop from   to tioga pass and back around to twain harte on his motorcycle  takes him about  hours   \n2391  with lakes and meadows everywhere this road is the drive in heaven  we took it from lee vining to the olmstead point  we wish to go further the next time                                                                                   ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_lemma</th>\n      <th>review_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2681</th>\n      <td>the park be huge    realize that  can not be experience in just a couple of day   the pass be magnificent    husband love to do the loop from    to tioga pass and back around to twain harte on  motorcycle   take  about   hour</td>\n      <td>the park is huge  you realize that it cant be experience in just a couple of days  the pass is magnificent  my husband loves to do the loop from   to tioga pass and back around to twain harte on his motorcycle  takes him about  hours</td>\n    </tr>\n    <tr>\n      <th>2391</th>\n      <td>with lake and meadow everywhere this road be the drive in heaven    take  from lee vine to the olmstead point    wish to go further the next time</td>\n      <td>with lakes and meadows everywhere this road is the drive in heaven  we took it from lee vining to the olmstead point  we wish to go further the next time</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 369
    }
   ],
   "source": [
    "df[['review_lemma','review_clean']].sample(2)"
   ]
  },
  {
   "source": [
    "Final level of filtering "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[&#39;i&#39;,\n &#39;me&#39;,\n &#39;my&#39;,\n &#39;myself&#39;,\n &#39;we&#39;,\n &#39;our&#39;,\n &#39;ours&#39;,\n &#39;ourselves&#39;,\n &#39;you&#39;,\n &quot;you&#39;re&quot;,\n &quot;you&#39;ve&quot;,\n &quot;you&#39;ll&quot;,\n &quot;you&#39;d&quot;,\n &#39;your&#39;,\n &#39;yours&#39;,\n &#39;yourself&#39;,\n &#39;yourselves&#39;,\n &#39;he&#39;,\n &#39;him&#39;,\n &#39;his&#39;,\n &#39;himself&#39;,\n &#39;she&#39;,\n &quot;she&#39;s&quot;,\n &#39;her&#39;,\n &#39;hers&#39;,\n &#39;herself&#39;,\n &#39;it&#39;,\n &quot;it&#39;s&quot;,\n &#39;its&#39;,\n &#39;itself&#39;,\n &#39;they&#39;,\n &#39;them&#39;,\n &#39;their&#39;,\n &#39;theirs&#39;,\n &#39;themselves&#39;,\n &#39;what&#39;,\n &#39;which&#39;,\n &#39;who&#39;,\n &#39;whom&#39;,\n &#39;this&#39;,\n &#39;that&#39;,\n &quot;that&#39;ll&quot;,\n &#39;these&#39;,\n &#39;those&#39;,\n &#39;am&#39;,\n &#39;is&#39;,\n &#39;are&#39;,\n &#39;was&#39;,\n &#39;were&#39;,\n &#39;be&#39;,\n &#39;been&#39;,\n &#39;being&#39;,\n &#39;have&#39;,\n &#39;has&#39;,\n &#39;had&#39;,\n &#39;having&#39;,\n &#39;do&#39;,\n &#39;does&#39;,\n &#39;did&#39;,\n &#39;doing&#39;,\n &#39;a&#39;,\n &#39;an&#39;,\n &#39;the&#39;,\n &#39;and&#39;,\n &#39;but&#39;,\n &#39;if&#39;,\n &#39;or&#39;,\n &#39;because&#39;,\n &#39;as&#39;,\n &#39;until&#39;,\n &#39;while&#39;,\n &#39;of&#39;,\n &#39;at&#39;,\n &#39;by&#39;,\n &#39;for&#39;,\n &#39;with&#39;,\n &#39;about&#39;,\n &#39;against&#39;,\n &#39;between&#39;,\n &#39;into&#39;,\n &#39;through&#39;,\n &#39;during&#39;,\n &#39;before&#39;,\n &#39;after&#39;,\n &#39;above&#39;,\n &#39;below&#39;,\n &#39;to&#39;,\n &#39;from&#39;,\n &#39;up&#39;,\n &#39;down&#39;,\n &#39;in&#39;,\n &#39;out&#39;,\n &#39;on&#39;,\n &#39;off&#39;,\n &#39;over&#39;,\n &#39;under&#39;,\n &#39;again&#39;,\n &#39;further&#39;,\n &#39;then&#39;,\n &#39;once&#39;,\n &#39;here&#39;,\n &#39;there&#39;,\n &#39;when&#39;,\n &#39;where&#39;,\n &#39;why&#39;,\n &#39;how&#39;,\n &#39;all&#39;,\n &#39;any&#39;,\n &#39;both&#39;,\n &#39;each&#39;,\n &#39;few&#39;,\n &#39;more&#39;,\n &#39;most&#39;,\n &#39;other&#39;,\n &#39;some&#39;,\n &#39;such&#39;,\n &#39;no&#39;,\n &#39;nor&#39;,\n &#39;not&#39;,\n &#39;only&#39;,\n &#39;own&#39;,\n &#39;same&#39;,\n &#39;so&#39;,\n &#39;than&#39;,\n &#39;too&#39;,\n &#39;very&#39;,\n &#39;s&#39;,\n &#39;t&#39;,\n &#39;can&#39;,\n &#39;will&#39;,\n &#39;just&#39;,\n &#39;don&#39;,\n &quot;don&#39;t&quot;,\n &#39;should&#39;,\n &quot;should&#39;ve&quot;,\n &#39;now&#39;,\n &#39;d&#39;,\n &#39;ll&#39;,\n &#39;m&#39;,\n &#39;o&#39;,\n &#39;re&#39;,\n &#39;ve&#39;,\n &#39;y&#39;,\n &#39;ain&#39;,\n &#39;aren&#39;,\n &quot;aren&#39;t&quot;,\n &#39;couldn&#39;,\n &quot;couldn&#39;t&quot;,\n &#39;didn&#39;,\n &quot;didn&#39;t&quot;,\n &#39;doesn&#39;,\n &quot;doesn&#39;t&quot;,\n &#39;hadn&#39;,\n &quot;hadn&#39;t&quot;,\n &#39;hasn&#39;,\n &quot;hasn&#39;t&quot;,\n &#39;haven&#39;,\n &quot;haven&#39;t&quot;,\n &#39;isn&#39;,\n &quot;isn&#39;t&quot;,\n &#39;ma&#39;,\n &#39;mightn&#39;,\n &quot;mightn&#39;t&quot;,\n &#39;mustn&#39;,\n &quot;mustn&#39;t&quot;,\n &#39;needn&#39;,\n &quot;needn&#39;t&quot;,\n &#39;shan&#39;,\n &quot;shan&#39;t&quot;,\n &#39;shouldn&#39;,\n &quot;shouldn&#39;t&quot;,\n &#39;wasn&#39;,\n &quot;wasn&#39;t&quot;,\n &#39;weren&#39;,\n &quot;weren&#39;t&quot;,\n &#39;won&#39;,\n &quot;won&#39;t&quot;,\n &#39;wouldn&#39;,\n &quot;wouldn&#39;t&quot;]"
     },
     "metadata": {},
     "execution_count": 370
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list\n",
    "\n",
    "words_count = FreqDist(df.review_lemma[1])\n",
    "\n",
    "long_words = [w for w in words_count if len(w) > 15]\n",
    "small_words = [w for w in words_count if len(w) < 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Reviews_cleaned_for_NLP.csv\", index = False)"
   ]
  },
  {
   "source": [
    "### Tokenization "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[&#39;Hi&#39;, &#39;Mr.&#39;, &#39;Smith&#39;, &#39;!&#39;, &#39;I&#39;, &#39;’&#39;, &#39;m&#39;, &#39;going&#39;, &#39;to&#39;, &#39;buy&#39;, &#39;some&#39;, &#39;vegetables&#39;, &#39;(&#39;, &#39;tomatoes&#39;, &#39;and&#39;, &#39;cucumbers&#39;, &#39;)&#39;, &#39;from&#39;, &#39;the&#39;, &#39;store&#39;, &#39;.&#39;, &#39;Should&#39;, &#39;I&#39;, &#39;pick&#39;, &#39;up&#39;, &#39;some&#39;, &#39;black-eyed&#39;, &#39;peas&#39;, &#39;as&#39;, &#39;well&#39;, &#39;?&#39;]\n[(&#39;Hi&#39;, &#39;Mr.&#39;), (&#39;Mr.&#39;, &#39;Smith&#39;), (&#39;Smith&#39;, &#39;!&#39;), (&#39;!&#39;, &#39;I&#39;), (&#39;I&#39;, &#39;’&#39;), (&#39;’&#39;, &#39;m&#39;), (&#39;m&#39;, &#39;going&#39;), (&#39;going&#39;, &#39;to&#39;), (&#39;to&#39;, &#39;buy&#39;), (&#39;buy&#39;, &#39;some&#39;), (&#39;some&#39;, &#39;vegetables&#39;), (&#39;vegetables&#39;, &#39;(&#39;), (&#39;(&#39;, &#39;tomatoes&#39;), (&#39;tomatoes&#39;, &#39;and&#39;), (&#39;and&#39;, &#39;cucumbers&#39;), (&#39;cucumbers&#39;, &#39;)&#39;), (&#39;)&#39;, &#39;from&#39;), (&#39;from&#39;, &#39;the&#39;), (&#39;the&#39;, &#39;store&#39;), (&#39;store&#39;, &#39;.&#39;), (&#39;.&#39;, &#39;Should&#39;), (&#39;Should&#39;, &#39;I&#39;), (&#39;I&#39;, &#39;pick&#39;), (&#39;pick&#39;, &#39;up&#39;), (&#39;up&#39;, &#39;some&#39;), (&#39;some&#39;, &#39;black-eyed&#39;), (&#39;black-eyed&#39;, &#39;peas&#39;), (&#39;peas&#39;, &#39;as&#39;), (&#39;as&#39;, &#39;well&#39;), (&#39;well&#39;, &#39;?&#39;)]\n[&#39;Hi&#39;, &#39;Mr.&#39;, &#39;Smith!&#39;, &#39;I’m&#39;, &#39;going&#39;, &#39;to&#39;, &#39;buy&#39;, &#39;some&#39;, &#39;vegetables&#39;, &#39;(tomatoes&#39;, &#39;and&#39;, &#39;cucumbers)&#39;, &#39;from&#39;, &#39;the&#39;, &#39;store.&#39;, &#39;Should&#39;, &#39;I&#39;, &#39;pick&#39;, &#39;up&#39;, &#39;some&#39;, &#39;black-eyed&#39;, &#39;peas&#39;, &#39;as&#39;, &#39;well?&#39;]\n[&#39;Hi&#39;, &#39;Mr&#39;, &#39;Smith&#39;, &#39;Should&#39;]\n"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from \\\n",
    "the store. Should I pick up some black-eyed peas as well?\"\n",
    "\n",
    "print(word_tokenize(my_text))\n",
    "\n",
    "# (N-Grams)\n",
    "\n",
    "from nltk.util import ngrams\n",
    "my_words = word_tokenize(my_text) # This is the list of all words\n",
    "twograms = list(ngrams(my_words,2)) # This is for two-word combos, but can pick any n\n",
    "print(twograms)\n",
    "\n",
    "# Regular Expressions\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# RegexpTokenizer with whitespace delimiter\n",
    "whitespace_tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "print(whitespace_tokenizer.tokenize(my_text))\n",
    "\n",
    "# RegexpTokenizer to match only capitalized words\n",
    "cap_tokenizer = RegexpTokenizer(\"[A-Z]['\\w]+\")\n",
    "print(cap_tokenizer.tokenize(my_text))\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "regexp_tokenize(s, pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "wordpunct_tokenize(s)\n",
    "\n",
    "blankline_tokenize(s)\n"
   ]
  },
  {
   "source": [
    "### Preprocessing: Stop Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "#Example impact with code\n",
    "\n",
    "my_text = [\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from \\\n",
    "the store. Should I pick up some black-eyed peas as well?\"]\n",
    "\n",
    "# Incorporate stop words when creating the count vectorizer\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(my_text)\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "source": [
    "### POS Tagging With NLTK"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(&#39;James&#39;, &#39;NNP&#39;), (&#39;Smith&#39;, &#39;NNP&#39;), (&#39;lives&#39;, &#39;VBZ&#39;), (&#39;in&#39;, &#39;IN&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;United&#39;, &#39;NNP&#39;), (&#39;States&#39;, &#39;NNPS&#39;), (&#39;.&#39;, &#39;.&#39;)]\n"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text))\n",
    "print(tokens)\n",
    "\n",
    "#For help on the codes, use the below\n",
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "source": [
    "### Named Entity Recognition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text)) # this labels each word as a part of speech\n",
    "entities = ne_chunk(tokens) # this extracts entities from the list of words\n",
    "# help(entities)"
   ]
  },
  {
   "source": [
    "### Compound Term Extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer # multi-word expression\n",
    "my_text = \"You all are the greatest students of all time.\"\n",
    "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "mwe_tokens"
   ]
  }
 ]
}