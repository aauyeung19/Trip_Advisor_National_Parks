{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('base': conda)",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fdce1bee3a4df8d1a1fa4d38b29664ec97e28d1bea539720d01ec921b2621e44"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     attraction_name   attraction_id           user_name  \\\n6970     Vernal Fall  g61000-d483481  Fairport Travelers   \n6971     Vernal Fall  g61000-d483481      CAtravelfamily   \n6972     Vernal Fall  g61000-d483481        doodlebugakj   \n6973     Vernal Fall  g61000-d483481            Jase2153   \n6974     Vernal Fall  g61000-d483481          booradley2   \n\n            user_profile_link review_date  helpful_votes  rating  \\\n6970       /Profile/Clarkvara    Jul 2008          560.0       4   \n6971  /Profile/CAtravelfamily    Jun 2008           16.0       5   \n6972    /Profile/doodlebugakj    Jul 2007            NaN       5   \n6973        /Profile/Jase2153    Sep 2005            9.0       5   \n6974      /Profile/booradley2    Sep 2004          214.0       5   \n\n                                            review_link  \\\n6970  /ShowUserReviews-g61000-d483481-r17854214-Vern...   \n6971  /ShowUserReviews-g61000-d483481-r17268549-Vern...   \n6972  /ShowUserReviews-g61000-d483481-r8255121-Verna...   \n6973  /ShowUserReviews-g61000-d483481-r3910762-Verna...   \n6974  /ShowUserReviews-g61000-d483481-r2512847-Verna...   \n\n                                            review_text  \\\n6970  Vernal Falls is sort of like the first leg of ...   \n6971  Whew, it was a tough climb at times, but once ...   \n6972  This was a really fun hike but when we came ar...   \n6973  I was visiting Yosemite from Australia and wen...   \n6974  I&#39;ve never been especially enthusiastic about ...   \n\n                       review_title experience_date  \n6970         Nice not too long hike             NaN  \n6971              Worth the effort!             NaN  \n6972  Wow that was a lot of stairs!             NaN  \n6973                 Worth The Trip             NaN  \n6974        Do Not Miss Vernal Fall             NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attraction_name</th>\n      <th>attraction_id</th>\n      <th>user_name</th>\n      <th>user_profile_link</th>\n      <th>review_date</th>\n      <th>helpful_votes</th>\n      <th>rating</th>\n      <th>review_link</th>\n      <th>review_text</th>\n      <th>review_title</th>\n      <th>experience_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6970</th>\n      <td>Vernal Fall</td>\n      <td>g61000-d483481</td>\n      <td>Fairport Travelers</td>\n      <td>/Profile/Clarkvara</td>\n      <td>Jul 2008</td>\n      <td>560.0</td>\n      <td>4</td>\n      <td>/ShowUserReviews-g61000-d483481-r17854214-Vern...</td>\n      <td>Vernal Falls is sort of like the first leg of ...</td>\n      <td>Nice not too long hike</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6971</th>\n      <td>Vernal Fall</td>\n      <td>g61000-d483481</td>\n      <td>CAtravelfamily</td>\n      <td>/Profile/CAtravelfamily</td>\n      <td>Jun 2008</td>\n      <td>16.0</td>\n      <td>5</td>\n      <td>/ShowUserReviews-g61000-d483481-r17268549-Vern...</td>\n      <td>Whew, it was a tough climb at times, but once ...</td>\n      <td>Worth the effort!</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6972</th>\n      <td>Vernal Fall</td>\n      <td>g61000-d483481</td>\n      <td>doodlebugakj</td>\n      <td>/Profile/doodlebugakj</td>\n      <td>Jul 2007</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>/ShowUserReviews-g61000-d483481-r8255121-Verna...</td>\n      <td>This was a really fun hike but when we came ar...</td>\n      <td>Wow that was a lot of stairs!</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6973</th>\n      <td>Vernal Fall</td>\n      <td>g61000-d483481</td>\n      <td>Jase2153</td>\n      <td>/Profile/Jase2153</td>\n      <td>Sep 2005</td>\n      <td>9.0</td>\n      <td>5</td>\n      <td>/ShowUserReviews-g61000-d483481-r3910762-Verna...</td>\n      <td>I was visiting Yosemite from Australia and wen...</td>\n      <td>Worth The Trip</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6974</th>\n      <td>Vernal Fall</td>\n      <td>g61000-d483481</td>\n      <td>booradley2</td>\n      <td>/Profile/booradley2</td>\n      <td>Sep 2004</td>\n      <td>214.0</td>\n      <td>5</td>\n      <td>/ShowUserReviews-g61000-d483481-r2512847-Verna...</td>\n      <td>I've never been especially enthusiastic about ...</td>\n      <td>Do Not Miss Vernal Fall</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "df = pd.read_csv(\"attraction_point_reviews.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "source": [
    "### Tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from \\\n",
    "the store. Should I pick up some black-eyed peas as well?\"\n",
    "\n",
    "print(word_tokenize(my_text))"
   ]
  },
  {
   "source": [
    "### Tokenization (N-Grams)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(&#39;James&#39;, &#39;Smith&#39;), (&#39;Smith&#39;, &#39;lives&#39;), (&#39;lives&#39;, &#39;in&#39;), (&#39;in&#39;, &#39;the&#39;), (&#39;the&#39;, &#39;United&#39;), (&#39;United&#39;, &#39;States&#39;), (&#39;States&#39;, &#39;.&#39;)]\n"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "my_words = word_tokenize(my_text) # This is the list of all words\n",
    "twograms = list(ngrams(my_words,2)) # This is for two-word combos, but can pick any n\n",
    "print(twograms)"
   ]
  },
  {
   "source": [
    "### Tokenization (Regular Expressions)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[&#39;James&#39;, &#39;Smith&#39;, &#39;lives&#39;, &#39;in&#39;, &#39;the&#39;, &#39;United&#39;, &#39;States.&#39;]\n[&#39;James&#39;, &#39;Smith&#39;, &#39;United&#39;, &#39;States&#39;]\n"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# RegexpTokenizer with whitespace delimiter\n",
    "whitespace_tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "print(whitespace_tokenizer.tokenize(my_text))\n",
    "\n",
    "\n",
    "# RegexpTokenizer to match only capitalized words\n",
    "cap_tokenizer = RegexpTokenizer(\"[A-Z]['\\w]+\")\n",
    "print(cap_tokenizer.tokenize(my_text))"
   ]
  },
  {
   "source": [
    "### Other Misc. Cleanups"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Regular expression library\n",
    "import string\n",
    "\n",
    "# Replace punctuations with a white space\n",
    "clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)\n",
    "clean_text\n",
    "\n",
    "clean_text = clean_text.lower()\n",
    "clean_text\n",
    "\n",
    "# Removes all words containing digits\n",
    "clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
    "clean_text"
   ]
  },
  {
   "source": [
    "### Preprocessing: Stop Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "#Example impact with code\n",
    "\n",
    "my_text = [\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from \\\n",
    "the store. Should I pick up some black-eyed peas as well?\"]\n",
    "\n",
    "# Incorporate stop words when creating the count vectorizer\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(my_text)\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "source": [
    "### Preprocessing: Stemming & Lemmatizing\n",
    "\n",
    "Can use PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "# Try some stems\n",
    "print('drive: {}’.format(stemmer.stem('drive')))\n",
    "print('drives: {}'.format(stemmer.stem('drives')))\n",
    "print('driver: {}'.format(stemmer.stem('driver')))\n",
    "print('drivers: {}'.format(stemmer.stem('drivers')))\n",
    "print('driven: {}'.format(stemmer.stem('driven')))"
   ]
  },
  {
   "source": [
    "### POS Tagging With NLTK"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(&#39;James&#39;, &#39;NNP&#39;), (&#39;Smith&#39;, &#39;NNP&#39;), (&#39;lives&#39;, &#39;VBZ&#39;), (&#39;in&#39;, &#39;IN&#39;), (&#39;the&#39;, &#39;DT&#39;), (&#39;United&#39;, &#39;NNP&#39;), (&#39;States&#39;, &#39;NNPS&#39;), (&#39;.&#39;, &#39;.&#39;)]\n"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text))\n",
    "print(tokens)\n",
    "\n",
    "#For help on the codes, use the below\n",
    "# nltk.help.upenn_tagset()"
   ]
  },
  {
   "source": [
    "### Named Entity Recognition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text)) # this labels each word as a part of speech\n",
    "entities = ne_chunk(tokens) # this extracts entities from the list of words\n",
    "# help(entities)"
   ]
  },
  {
   "source": [
    "### Compound Term Extraction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer # multi-word expression\n",
    "my_text = \"You all are the greatest students of all time.\"\n",
    "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "mwe_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}